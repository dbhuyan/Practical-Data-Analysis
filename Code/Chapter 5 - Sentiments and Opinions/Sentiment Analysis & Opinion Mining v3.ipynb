{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis & Opinion Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spelling Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install pytypo\n",
    "\n",
    "Other spelling correction libraries - \"topy\", \"hunspell\" and \"autocorrect\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This iPhone thing is really cool.\n"
     ]
    }
   ],
   "source": [
    "sentence = 'This iPhone thng isss realllllllllly coooolll.......'\n",
    "\n",
    "import pytypo\n",
    "sentenceX = pytypo.correct_sentence(sentence)\n",
    "print(sentenceX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis using SentiWordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import *\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "## Convert from PennTreebank tags to Wordnet tags\n",
    "def pennTOwn(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "## My Sentiment Analysis Logic\n",
    "def mySAL(sentence):\n",
    "    pscore = 0\n",
    "    nscore = 0\n",
    "    oscore = 0\n",
    "    wordcount = 0\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "    print(tagged)\n",
    "    \n",
    "    for tag in tagged:\n",
    "        \n",
    "        lemma = ''\n",
    "        pos = pennTOwn(tag[1])\n",
    "    \n",
    "        if (pos != None):\n",
    "            lemma = lemmatizer.lemmatize(tag[0], pos = pos)\n",
    "            wordcount+= 1\n",
    "           \n",
    "        if 'VB' in tag[1] and len(list(swn.senti_synsets(lemma,'v'))) > 0:\n",
    "            tempSynsets = list(swn.senti_synsets(lemma,'v'))\n",
    "            P = tempSynsets[0].pos_score()\n",
    "            N = tempSynsets[0].neg_score()\n",
    "            O = tempSynsets[0].obj_score()\n",
    "            pscore+= P\n",
    "            nscore+= N\n",
    "            oscore+= O\n",
    "            output = lemma + \" -- P: \" + str(P) + \", N: \" + str(N) + \", O: \" + str(O)\n",
    "            print(output)\n",
    "        \n",
    "        elif 'JJ' in tag[1] and len(list(swn.senti_synsets(lemma,'a'))) > 0:\n",
    "            tempSynsets = list(swn.senti_synsets(lemma,'a'))\n",
    "            P = tempSynsets[0].pos_score()\n",
    "            N = tempSynsets[0].neg_score()\n",
    "            O = tempSynsets[0].obj_score()\n",
    "            pscore+= P\n",
    "            nscore+= N\n",
    "            oscore+= O\n",
    "            output = lemma + \" -- P: \" + str(P) + \", N: \" + str(N) + \", O: \" + str(O)\n",
    "            print(output)\n",
    "        \n",
    "        elif 'RB' in tag[1] and len(list(swn.senti_synsets(lemma,'r'))) > 0:\n",
    "            tempSynsets = list(swn.senti_synsets(lemma,'r'))\n",
    "            P = tempSynsets[0].pos_score()\n",
    "            N = tempSynsets[0].neg_score()\n",
    "            O = tempSynsets[0].obj_score()\n",
    "            pscore+= P\n",
    "            nscore+= N\n",
    "            oscore+= O\n",
    "            output = lemma + \" -- P: \" + str(P) + \", N: \" + str(N) + \", O: \" + str(O)\n",
    "            print(output)\n",
    "            \n",
    "    print(\"Positive:\", pscore, \" Negative:\", nscore, \" Objectivity:\", oscore)\n",
    "    return pscore, nscore, oscore, wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('love', 'VBP'), ('eating', 'VBG'), ('this', 'DT'), ('wonderful', 'JJ'), ('dish', 'NN'), ('.', '.')]\n",
      "love -- P: 0.5, N: 0.0, O: 0.5\n",
      "eat -- P: 0.0, N: 0.0, O: 1.0\n",
      "wonderful -- P: 0.75, N: 0.0, O: 0.25\n",
      "Positive: 1.25  Negative: 0.0  Objectivity: 1.75\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I love eating this wonderful dish.\"\n",
    "P, N, O, C = mySAL(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full list of TreeBank Part of Speech tags.\n",
    "\n",
    "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('VBP')\n",
    "nltk.help.upenn_tagset('VBG')\n",
    "nltk.help.upenn_tagset('JJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('do', 'VBP'), ('not', 'RB'), ('like', 'IN'), ('this', 'DT'), ('dish', 'NN'), ('.', '.')]\n",
      "do -- P: 0.0, N: 0.0, O: 1.0\n",
      "not -- P: 0.0, N: 0.625, O: 0.375\n",
      "Positive: 0.0  Negative: 0.625  Objectivity: 1.375\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I do not like this dish.\"\n",
    "P, N, O, C = mySAL(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('do', 'VBP'), (\"n't\", 'RB'), ('like', 'VB'), ('this', 'DT'), ('dish', 'NN'), ('.', '.')]\n",
      "do -- P: 0.0, N: 0.0, O: 1.0\n",
      "like -- P: 0.125, N: 0.0, O: 0.875\n",
      "Positive: 0.125  Negative: 0.0  Objectivity: 1.875\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I don't like this dish.\"\n",
    "P, N, O, C = mySAL(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WikiPedia List of Contractions\n",
    "\n",
    "https://en.wikipedia.org/wiki/Wikipedia%3aList_of_English_contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "contractions_dict = { \n",
    "\"can't\": \"cannot\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "\n",
    "def expand_contractions(sentence):\n",
    "    def replace(match):\n",
    "        return contractions_dict[match.group(0)]\n",
    "\n",
    "    return contractions_re.sub(replace, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('do', 'VBP'), ('not', 'RB'), ('like', 'IN'), ('this', 'DT'), ('dish', 'NN'), ('.', '.')]\n",
      "do -- P: 0.0, N: 0.0, O: 1.0\n",
      "not -- P: 0.0, N: 0.625, O: 0.375\n",
      "Positive: 0.0  Negative: 0.625  Objectivity: 1.375\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I don't like this dish.\"\n",
    "sentenceX = expand_contractions(sentence)\n",
    "P, N, O, C = mySAL(sentenceX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring formulas:\n",
    "\n",
    "1) Absolute Proportional Difference. Bounds: [0,1]\n",
    "    Sentiment = (P − N) / (Word Count)\n",
    "\n",
    "    Disadvantage: A sentence's score is affected by non-sentiment-related content.\n",
    "\n",
    "2) Relative Proportional Difference. Bounds: [-1, 1]\n",
    "\n",
    "    Sentiment = (P − N) / (P + N)\n",
    "\n",
    "    Disadvantage: A sentence's score may tend to cluster very strongly near the scale endpoints (because they may contain content primarily or exclusively of either positive or negative).\n",
    "\n",
    "3) Logit scale. Bounds: [-infinity, +infinity]\n",
    "\n",
    "    Sentiment = log(P + 0.5) - log(N + 0.5)\n",
    "    \n",
    "    This tends to have the smoothest properties and is symmetric around zero. The 0.5 is a smoother to prevent log(0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sentiScore(P, N, O, C):\n",
    "    APD = RPD = LS = 0\n",
    "    \n",
    "    if (P + N + O) > 0:\n",
    "        APD = (P - N) / C\n",
    "        \n",
    "    if (P + N) > 0:\n",
    "        RPD = (P - N) / (P + N)\n",
    "    \n",
    "    LS = np.log10(P + 0.5) - np.log(N + 0.5)\n",
    "    \n",
    "    print(\"APD:\", APD, \" RPD:\", RPD, \" LS:\", LS)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('hate', 'VBP'), ('the', 'DT'), ('new', 'JJ'), ('phone', 'NN'), ('.', '.')]\n",
      "hate -- P: 0.0, N: 0.75, O: 0.25\n",
      "new -- P: 0.375, N: 0.0, O: 0.625\n",
      "Positive: 0.375  Negative: 0.75  Objectivity: 0.875\n",
      "APD: -0.125  RPD: -0.3333333333333333  LS: -0.281135498292\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I hate the new phone.\"\n",
    "sentenceX = expand_contractions(sentence)\n",
    "P, N, O, C = mySAL(sentenceX)\n",
    "sentiScore(P, N, O, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('price', 'NN'), ('of', 'IN'), ('the', 'DT'), ('new', 'JJ'), ('iPhone', 'NN'), ('is', 'VBZ'), ('terrible', 'JJ'), ('.', '.')]\n",
      "new -- P: 0.375, N: 0.0, O: 0.625\n",
      "be -- P: 0.25, N: 0.125, O: 0.625\n",
      "terrible -- P: 0.0, N: 0.625, O: 0.375\n",
      "Positive: 0.625  Negative: 0.75  Objectivity: 1.625\n",
      "APD: -0.025  RPD: -0.09090909090909091  LS: -0.171991028867\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The price of the new iPhone is terrible.\"\n",
    "sentenceX = expand_contractions(sentence)\n",
    "P, N, O, C = mySAL(sentenceX)\n",
    "sentiScore(P, N, O, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VADER - A rule-based model for sentiment analysis\n",
    "\n",
    "http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf\n",
    "\n",
    "Citation - Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.\n",
    "\n",
    "Note - \"twython\" Python package and NLTK add-ons needs to be installed.\n",
    "\n",
    "NLTK add-ons can be installed by:\n",
    "\n",
    "import nltk\n",
    "nltk.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "\n",
    "def myVader(sentence):\n",
    "    sid = SIA()\n",
    "    score = sid.polarity_scores(sentence)\n",
    "    \n",
    "    if (score['compound'] > 0):\n",
    "        sentiment = \"Positive\"\n",
    "    elif (score['compound'] < 0):\n",
    "        sentiment = \"Negative\"\n",
    "    else:\n",
    "        sentiment = \"Neutral\"\n",
    "\n",
    "    print(\"Overall Sentiment - \", sentiment)\n",
    "    print(\"Sentiment Score - \", score)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Sentiment -  Negative\n",
      "Sentiment Score -  {'neg': 0.136, 'neu': 0.864, 'pos': 0.0, 'compound': -0.6249}\n"
     ]
    }
   ],
   "source": [
    "sentence = \"\"\"Potentially the worst written book I've read since 2005. You can learn python NLTK library IF \n",
    "you already know NLP concepts. You can't learn NLP from it.\"\"\"\n",
    "\n",
    "myVader(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Sentiment -  Positive\n",
      "Sentiment Score -  {'neg': 0.0, 'neu': 0.702, 'pos': 0.298, 'compound': 0.9432}\n"
     ]
    }
   ],
   "source": [
    "sentence = \"\"\"This book is wonderfully written. It's full of interesting examples, and gradually and \n",
    "effortlessly introduces the reader to quite advanced topics in Natural Language Processing, Python, and \n",
    "machine learning. One of the few technical book that is worth reading cover-to-cover, just for \n",
    "the pleasure of it.\"\"\"\n",
    "\n",
    "myVader(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Sentiment -  Neutral\n",
      "Sentiment Score -  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n"
     ]
    }
   ],
   "source": [
    "sentence = \"This is a book.\"\n",
    "myVader(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Sentiment -  Positive\n",
      "Sentiment Score -  {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.3182}\n"
     ]
    }
   ],
   "source": [
    "sentence = \":-)\"\n",
    "myVader(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Sentiment -  Negative\n",
      "Sentiment Score -  {'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound': -0.3612}\n"
     ]
    }
   ],
   "source": [
    "sentence = \":-(\"\n",
    "myVader(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Sentiment -  Positive\n",
      "Sentiment Score -  {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.4215}\n"
     ]
    }
   ],
   "source": [
    "sentence = \"LOL\"\n",
    "myVader(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognizer (NER)\n",
    "\n",
    "Citation: Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL 2005), pp. 363-370. http://nlp.stanford.edu/~manning/papers/gibbscrf3.pdf\n",
    "\n",
    "Stanford NER - https://nlp.stanford.edu/software/CRF-NER.shtml#Download\n",
    "\n",
    "Accuracy:\n",
    "https://pythonprogramming.net/testing-stanford-ner-taggers-for-accuracy/?completed=/named-entity-recognition-stanford-ner-tagger/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.chunk import conlltags2tree\n",
    "from nltk.tree import Tree\n",
    "import os\n",
    "\n",
    "java_path = 'C:/Program Files (x86)/Java/jre1.8.0_121/bin/java.exe'\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "classifier = 'D:/Stanford/stanford-ner-2017-06-09/classifiers/english.all.3class.distsim.crf.ser.gz'\n",
    "jar = 'D:/Stanford/stanford-ner-2017-06-09/stanford-ner.jar'\n",
    "\n",
    "# Convert stanford NE to NLTK tree\n",
    "def stanfordNE2BIO(tagged_sent):\n",
    "    bio_tagged_sent = []\n",
    "    prev_tag = \"O\"\n",
    "    for token, tag in tagged_sent:\n",
    "        if tag == \"O\": #O\n",
    "            bio_tagged_sent.append((token, tag))\n",
    "            prev_tag = tag\n",
    "            continue\n",
    "        if tag != \"O\" and prev_tag == \"O\": # Begin NE\n",
    "            bio_tagged_sent.append((token, \"B-\"+tag))\n",
    "            prev_tag = tag\n",
    "        elif prev_tag != \"O\" and prev_tag == tag: # Inside NE\n",
    "            bio_tagged_sent.append((token, \"I-\"+tag))\n",
    "            prev_tag = tag\n",
    "        elif prev_tag != \"O\" and prev_tag != tag: # Adjacent NE\n",
    "            bio_tagged_sent.append((token, \"B-\"+tag))\n",
    "            prev_tag = tag\n",
    "\n",
    "    return bio_tagged_sent\n",
    "\n",
    "# Convert stanford NE to NLTK tree\n",
    "def stanfordNE2tree(ne_tagged_sent):\n",
    "    bio_tagged_sent = stanfordNE2BIO(ne_tagged_sent)\n",
    "    sent_tokens, sent_ne_tags = zip(*bio_tagged_sent)\n",
    "    sent_pos_tags = [pos for token, pos in pos_tag(sent_tokens)]\n",
    "\n",
    "    sent_conlltags = [(token, pos, ne) for token, pos, ne in zip(sent_tokens, sent_pos_tags, sent_ne_tags)]\n",
    "    ne_tree = conlltags2tree(sent_conlltags)\n",
    "    return ne_tree\n",
    "\n",
    "\n",
    "def myNER(sentence):\n",
    "    st = StanfordNERTagger(classifier, jar, encoding='utf-8')\n",
    "    tokens = word_tokenize(sentence)\n",
    "    classifiedTokens = st.tag(tokens)\n",
    "    \n",
    "    neTree = stanfordNE2tree(classifiedTokens)\n",
    "    \n",
    "    # To deal with situations like \"Donald\" and \"Trump\" is one Named Entity rather than two.\n",
    "    neJoined = []\n",
    "    for subtree in neTree:\n",
    "        if type(subtree) == Tree: # If subtree is a noun chunk, i.e. NE != \"O\"\n",
    "            ne_label = subtree.label()\n",
    "            ne_string = \" \".join([token for token, pos in subtree.leaves()])\n",
    "            neJoined.append((ne_string, ne_label))\n",
    "    \n",
    "    neList = []\n",
    "    \n",
    "    for token in neJoined:\n",
    "        if ((token[1] == 'PERSON') or (token[1] == 'ORGANIZATION') or (token[1] == 'LOCATION')):\n",
    "            neList.append(token[0])\n",
    "    \n",
    "    ## Use dict to remove duplicates (Python 3.6 only)\n",
    "    print(\"Named Entity List:\", list(dict.fromkeys(neList)))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "Could not find stanford-ner.jar jar file at D:/Stanford/stanford-ner-2017-06-09/stanford-ner.jar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-4887293c8d81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m in his words, “an athlete needs challenges.”\"\"\"\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmyNER\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mmyVader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-e15b3d21fe40>\u001b[0m in \u001b[0;36mmyNER\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmyNER\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStanfordNERTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mclassifiedTokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tag/stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStanfordNERTagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tag/stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_filename, path_to_jar, encoding, verbose, java_options)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_JAR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_jar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0msearchpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_stanford_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 verbose=verbose)\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         self._stanford_model = find_file(model_filename,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mfind_jar\u001b[0;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[1;32m    719\u001b[0m         searchpath=(), url=None, verbose=False, is_regex=False):\n\u001b[1;32m    720\u001b[0m     return next(find_jar_iter(name_pattern, path_to_jar, env_vars,\n\u001b[0;32m--> 721\u001b[0;31m                          searchpath, url, verbose, is_regex))\n\u001b[0m\u001b[1;32m    722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mfind_jar_iter\u001b[0;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[1;32m    635\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             raise LookupError('Could not find %s jar file at %s' %\n\u001b[0;32m--> 637\u001b[0;31m                             (name_pattern, path_to_jar))\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;31m# Check environment variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: Could not find stanford-ner.jar jar file at D:/Stanford/stanford-ner-2017-06-09/stanford-ner.jar"
     ]
    }
   ],
   "source": [
    "sentence = \"\"\"Neymar is now the highest salaried athlete on earth, but that doesn’t sound like his \n",
    "primary motivating factor in moving to Paris Saint-Germain. In a series of Instagram videos posted \n",
    "after the move, Neymar said that his father didn’t want him to leave Barcelona. He did it anyway because, \n",
    "in his words, “an athlete needs challenges.”\"\"\"\n",
    "\n",
    "myNER(sentence)\n",
    "myVader(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"Holidaymakers across Europe face sweltering temperatures as a “dangerous” \n",
    "heatwave hits the continent. Scorching temperatures are forecast for countries such as Spain, \n",
    "Italy and Croatia with the mercury set to hit 40C prompting severe warnings.\"\"\"\n",
    "\n",
    "myNER(sentence)\n",
    "myVader(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"Michael Gove has confirmed that some foreign trawlers will still have access to UK \n",
    "waters after Brexit. Mr Gove, the UK environment secretary, said British fishermen would not have \n",
    "the capacity to land all of the fish in British territorial waters. And he said that some access would\n",
    "therefore be granted to vessels from other countries. He was speaking during a fact-finding mission to \n",
    "Denmark, which was largely focused on the Danish food industry. The Danish fishing industry is currently \n",
    "highly-dependent on fish caught in UK territorial waters.\"\"\"\n",
    "\n",
    "myNER(sentence)\n",
    "myVader(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract main topics\n",
    "Original code written by Shlomi Babluki\n",
    "http://thetokenizer.com/2013/05/09/efficient-way-to-extract-the-main-topics-of-a-sentence/\n",
    "\n",
    "Modified to use Stanford POS tagger instead of NLTK POS. And skip the topic items already picked up by our NER model above.\n",
    "https://nlp.stanford.edu/software/tagger.shtml\n",
    "\n",
    "Citations for Stanford POS tagger:\n",
    "1) Kristina Toutanova and Christopher D. Manning. 2000. Enriching the Knowledge Sources Used in a Maximum Entropy Part-of-Speech Tagger. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-2000), pp. 63-70.\n",
    "\n",
    "2) Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network. In Proceedings of HLT-NAACL 2003, pp. 252-259. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import os\n",
    "java_path = 'C:/Program Files (x86)/Java/jre1.8.0_121/bin/java.exe'\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "model = 'D:/Stanford/stanford-postagger-full-2017-06-09/models/english-bidirectional-distsim.tagger'\n",
    "jarFile = 'D:/Stanford/stanford-postagger-full-2017-06-09/stanford-postagger.jar'\n",
    "\n",
    "# This is the semi-CFG\n",
    "##########################\n",
    "cfg = {}\n",
    "cfg[\"NNP+NNP\"] = \"NNP\"\n",
    "cfg[\"NN+NN\"] = \"NNI\"\n",
    "cfg[\"NNI+NN\"] = \"NNI\"\n",
    "cfg[\"JJ+JJ\"] = \"JJ\"\n",
    "cfg[\"JJ+NN\"] = \"NNI\"\n",
    "##########################\n",
    "\n",
    "\n",
    "def topicExtractor(sentence):\n",
    "\n",
    "    def normalize_tags(tagged):\n",
    "        n_tagged = []\n",
    "        for t in tagged:\n",
    "            if t[1].endswith(\"S\"):\n",
    "                n_tagged.append((t[0], t[1][:-1]))\n",
    "                continue\n",
    "            n_tagged.append((t[0], t[1]))\n",
    "        return n_tagged\n",
    "\n",
    "\n",
    "    tokens = word_tokenize(sentence)\n",
    "    myTagger = StanfordPOSTagger(model, jarFile)\n",
    "    tags = normalize_tags(myTagger.tag(tokens))\n",
    "\n",
    "    merge = True\n",
    "    while merge:\n",
    "        merge = False\n",
    "        for x in range(0, len(tags) - 1):\n",
    "            t1 = tags[x]\n",
    "            t2 = tags[x + 1]\n",
    "            key = \"%s+%s\" % (t1[1], t2[1])\n",
    "            value = cfg.get(key, '')\n",
    "            if value:\n",
    "                merge = True\n",
    "                tags.pop(x)\n",
    "                tags.pop(x)\n",
    "                match = \"%s %s\" % (t1[0], t2[0])\n",
    "                pos = value\n",
    "                tags.insert(x, (match, pos))\n",
    "                break\n",
    "\n",
    "    matches = []\n",
    "    for t in tags:\n",
    "        # Since our NER extractor already has the NNPs, we will only keep the NNIs\n",
    "        # If you want to keep the NNPs, include t[1] == \"NNP\" in the if logic below.\n",
    "        if t[1] == \"NNI\":\n",
    "            matches.append(t[0])\n",
    "        \n",
    "        # Remove duplicates\n",
    "        matches = list(dict.fromkeys(matches))\n",
    "    \n",
    "    print(\"Main Topic:\", matches)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"Michael Gove has confirmed that some foreign trawlers will still have access to UK \n",
    "waters after Brexit. Mr Gove, the UK environment secretary, said British fishermen would not have \n",
    "the capacity to land all of the fish in British territorial waters. And he said that some access would\n",
    "therefore be granted to vessels from other countries. He was speaking during a fact-finding mission to \n",
    "Denmark, which was largely focused on the Danish food industry. The Danish fishing industry is currently \n",
    "highly-dependent on fish caught in UK territorial waters.\"\"\"\n",
    "\n",
    "topicExtractor(sentence)\n",
    "myNER(sentence)\n",
    "myVader(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product aspect extraction\n",
    "\n",
    "Extract noun or noun phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import os\n",
    "java_path = 'C:/Program Files (x86)/Java/jre1.8.0_121/bin/java.exe'\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "model = 'D:/Stanford/stanford-postagger-full-2017-06-09/models/english-bidirectional-distsim.tagger'\n",
    "jarFile = 'D:/Stanford/stanford-postagger-full-2017-06-09/stanford-postagger.jar'\n",
    "\n",
    "def aspectExtract(sentence):\n",
    "    sentences = sent_tokenize(sentence)\n",
    "    \n",
    "    for sent in sentences:\n",
    "        previousWord=''\n",
    "        previousTag=''\n",
    "        currentWord=''\n",
    "        aspectList=[]\n",
    "\n",
    "        #Extracting Aspects\n",
    "        tokens = word_tokenize(sent)\n",
    "        myTagger = StanfordPOSTagger(model, jarFile)\n",
    "        tags = myTagger.tag(tokens)\n",
    "        \n",
    "        for word,tag in tags:\n",
    "            if(tag == 'NN' or tag == 'NNP'):\n",
    "                if(previousTag == 'NN' or previousTag == 'NNP'):\n",
    "                    currentWord= previousWord + ' ' + word\n",
    "                else:\n",
    "                    if (previousWord != ''):\n",
    "                        aspectList.append(previousWord)\n",
    "                \n",
    "                    currentWord = word\n",
    "\n",
    "            previousWord = currentWord\n",
    "            previousTag = tag\n",
    "        \n",
    "        aspectList.append(previousWord)\n",
    "    \n",
    "        print(\"Sentence -\", sent)\n",
    "        print(\"Aspect List:\", aspectList)\n",
    "        \n",
    "        # How about we extract the sentiment too?\n",
    "        myVader(sent)\n",
    "           \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The battery life of an iPhone is fantastic. But the price is not very good.\"\n",
    "aspectExtract(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decompose complex sentences\n",
    "Idea is to do sentiment analysis on individual phrases as different parts of a complex sentence may have different sentiments.\n",
    "\n",
    "Clause level Penn Treebank labels (S, SBAR, SBARQ, SQ).\n",
    "\n",
    "http://www.surdeanu.info/mihai/teaching/ista555-fall13/readings/PennTreebankConstituents.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.parse.stanford import StanfordParser\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tree import *\n",
    "import os\n",
    "\n",
    "java_path = 'C:/Program Files (x86)/Java/jre1.8.0_121/bin/java.exe'\n",
    "classPath = 'D:/Stanford/stanford-postagger-full-2017-06-09/stanford-postagger.jar:D:/Stanford/stanford-ner-2017-06-09/stanford-ner.jar:D:/Stanford/stanford-parser-full-2017-06-09/stanford-parser-3.8.0-models.jar:D:/Stanford/stanford-parser-full-2017-06-09/stanford-parser.jar'\n",
    "models = 'D:/Stanford/stanford-postagger-full-2017-06-09/models:D:\\Stanford\\stanford-ner-2017-06-09\\classifiers'\n",
    "\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "os.environ['CLASSPATH'] = classPath\n",
    "os.environ['STANFORD_MODELS'] = models\n",
    "\n",
    "  \n",
    "def traverseTree(tree):\n",
    "    treeChunks = []\n",
    "     \n",
    "    for subtree in reversed(list(tree.subtrees())):\n",
    "        if ((subtree.label() == 'S') or (subtree.label() == 'SBAR') or (subtree.label() == 'SBARQ') or (subtree.label() == 'SQ')):\n",
    "            treeChunks.append(' '.join(subtree.leaves()))\n",
    "            del tree[subtree.treeposition()]\n",
    "                        \n",
    "    # Remove any empty string\n",
    "    if '' in treeChunks:\n",
    "        treeChunks.remove('')\n",
    "    \n",
    "    return list(reversed(treeChunks))\n",
    "\n",
    "def ClauseExtraction(sentence):\n",
    "    myParser = StanfordParser('D:/Stanford/stanford-parser-full-2017-06-09/stanford-parser.jar', 'D:/Stanford/stanford-parser-full-2017-06-09/stanford-parser-3.8.0-models.jar', model_path=\"edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\")\n",
    "\n",
    "    sentences = sent_tokenize(sentence)\n",
    "    newSentences = []\n",
    "    \n",
    "    for sent in sentences:\n",
    "        tempSent = []\n",
    "        tagged = next(myParser.raw_parse(sent))\n",
    "        ptagged = ParentedTree.convert(tagged)\n",
    "\n",
    "        print(traverseTree(ptagged))\n",
    "\n",
    "    return    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The battery life of an iPhone is fantastic but the price is not very good\"\n",
    "ClauseExtraction(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"ABC cites the fact that chemical additives are banned in many countries and feels they may be banned in this state too\"\n",
    "ClauseExtraction(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw the tree structure of a POS tagged sentence for visual analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.parse.stanford import StanfordParser\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tree import *\n",
    "import os\n",
    "\n",
    "java_path = 'C:/Program Files (x86)/Java/jre1.8.0_121/bin/java.exe'\n",
    "classPath = 'D:/Stanford/stanford-postagger-full-2017-06-09/stanford-postagger.jar:D:/Stanford/stanford-ner-2017-06-09/stanford-ner.jar:D:/Stanford/stanford-parser-full-2017-06-09/stanford-parser-3.8.0-models.jar:D:/Stanford/stanford-parser-full-2017-06-09/stanford-parser.jar'\n",
    "models = 'D:/Stanford/stanford-postagger-full-2017-06-09/models:D:\\Stanford\\stanford-ner-2017-06-09\\classifiers'\n",
    "\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "os.environ['CLASSPATH'] = classPath\n",
    "os.environ['STANFORD_MODELS'] = models\n",
    "\n",
    "\n",
    "def drawTree(sentence):\n",
    "    myParser = StanfordParser('D:/Stanford/stanford-parser-full-2017-06-09/stanford-parser.jar', 'D:/Stanford/stanford-parser-full-2017-06-09/stanford-parser-3.8.0-models.jar', model_path=\"edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\")\n",
    "\n",
    "    sentences = sent_tokenize(sentence)\n",
    "    newSentences = []\n",
    "    \n",
    "    for sent in sentences:\n",
    "        tempSent = []\n",
    "        tagged = next(myParser.raw_parse(sent))\n",
    "        tagged.draw()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = \"ABC cites the fact that chemical additives are banned in many countries and feels they may be banned in this state too\"\n",
    "drawTree(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
