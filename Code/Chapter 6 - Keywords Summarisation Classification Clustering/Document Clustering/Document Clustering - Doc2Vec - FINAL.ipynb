{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2Vec for Document Clustering\n",
    "\n",
    "Dataset - http://mlg.ucd.ie/datasets/bbc.html\n",
    "\n",
    "Citation - D. Greene and P. Cunningham. \"Practical Solutions to the Problem of Diagonal Dominance in Kernel Document Clustering\", Proc. ICML 2006.\n",
    "\n",
    "Consists of 2225 documents from the BBC news website corresponding to stories in five topical areas from 2004-2005.\n",
    "Class Labels: 5 (business, entertainment, politics, sport, tech)\n",
    "- 510 business\n",
    "- 386 entertainment\n",
    "- 417 politics\n",
    "- 511 sports\n",
    "- 401 tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "documents = []\n",
    "documentReference = []\n",
    "documentCount = 0\n",
    "\n",
    "def dataCleanup(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\n', ' ')\n",
    "    \n",
    "    # Add space around punctuations\n",
    "    for char in ['.', '\"', ',', '(', ')', '!', '?', ';', ':']:\n",
    "        text = text.replace(char, ' ' + char + ' ')\n",
    "    \n",
    "    return text\n",
    "\n",
    "def buildDocument(folder, category):\n",
    "    allDocs = Path(folder).glob('**/*')\n",
    "    global documentCount\n",
    "\n",
    "    for doc in allDocs:\n",
    "        file = open(doc, \"r\")\n",
    "        data = file.read()\n",
    "        cleanData = dataCleanup(data)\n",
    "        \n",
    "        documents.append(cleanData)\n",
    "        \n",
    "        # Keep a record of category and filename\n",
    "        documentReference.append([])\n",
    "        documentReference[documentCount].append(category)\n",
    "        documentReference[documentCount].append(doc)\n",
    "        documentCount = documentCount + 1\n",
    "    \n",
    "    return\n",
    "\n",
    "buildDocument(\"bbc-fulltext/business\", \"business\")\n",
    "buildDocument(\"bbc-fulltext/entertainment\", \"entertainment\")\n",
    "buildDocument(\"bbc-fulltext/politics\", \"politics\")\n",
    "buildDocument(\"bbc-fulltext/sport\", \"sport\")\n",
    "buildDocument(\"bbc-fulltext/tech\", \"tech\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dhiraj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dhiraj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Doc2Vec Model Saved\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import doc2vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from collections import namedtuple\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "\n",
    "download('stopwords') # For stopword removal\n",
    "download('punkt') # For tokenizer\n",
    "\n",
    "def removeStopwords(text):\n",
    "    # Removing stopwords improved results with BBC news data, but test with and without stop words.\n",
    "    stop_words = stopwords.words('english')\n",
    "    text = [w for w in text if w not in stop_words]\n",
    "    text = [w for w in text if w.isalpha()]\n",
    "    return text\n",
    "\n",
    "def text2tokens(text):\n",
    "    text = text.lower()\n",
    "    wordList = word_tokenize(text)\n",
    "    wordList = removeStopwords(wordList)\n",
    "    return wordList\n",
    "\n",
    "doc2vec_corpus = []\n",
    "\n",
    "for i, text in enumerate(documents):\n",
    "    words = text2tokens(text)\n",
    "    tag = [i]\n",
    "    doc2vec_corpus.append(TaggedDocument(words=words, tags=tag))\n",
    "\n",
    "# The model parameters below can impact the outcome.\n",
    "# 1. Size - Vector size. 100 worked best with the BBC news data set. Tried various between 50 to 300 before choosing 100.\n",
    "# 2. Window - context window, i.e. the number of words on the left and right of a word that \n",
    "# defines a \"context\" for learning the meaning of the word. Context window of 1 gave the best result (tried between 1 and 10)\n",
    "# ..... probably due to the very small size of documents / vocabulary.\n",
    "\n",
    "model = doc2vec.Doc2Vec(doc2vec_corpus, size = 100, negative = 5, window = 1, iter = 20, min_count = 2, workers = 4, alpha=0.025, min_alpha=0.025)\n",
    "model.save(\"bbc_news_doc2vec.model\")\n",
    "print(\"Doc2Vec Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docVectors = []\n",
    "count = 0\n",
    "while (count < documentCount):\n",
    "    docVectors.append(model.docvecs[count])\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "num_clusters = 5\n",
    "\n",
    "km = KMeans(n_clusters = num_clusters, random_state = 99999)\n",
    "km.fit(docVectors)\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def wordFrequencyFilter(text, max_word_count, freq):\n",
    "    text = text.lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    wordList = []\n",
    "    for tag in tagged:\n",
    "        if ((tag[1] == 'NN') or (tag[1] == 'NNS') or (tag[1] == 'NNP') or (tag[1] == 'NNPS')):\n",
    "            tagLemma = lemmatizer.lemmatize(tag[0])\n",
    "            wordList.append(tagLemma)\n",
    "\n",
    "    freqDist = nltk.FreqDist(wordList)\n",
    "    common = freqDist.most_common(max_word_count)\n",
    "        \n",
    "    mainText = ''\n",
    "    \n",
    "    for word in common:\n",
    "        # Exclude words less than 2 characters long.\n",
    "        # Exclude words with frequency count greater than freq\n",
    "        # Only include alphabetic strings\n",
    "        \n",
    "        if ((word[0].isalpha()) and (len(word[0]) > 2) and ((word[1] < freq) == False)):\n",
    "            mainText = mainText + ' ' + word[0]\n",
    "    \n",
    "    return mainText\n",
    "\n",
    "def extractKeywords(file):\n",
    "    myFile = open(file, 'r')\n",
    "    data = myFile.read()\n",
    "    \n",
    "    # 10 most frequent words, minimum frequency 2\n",
    "    mainText = wordFrequencyFilter(data, 10, 2)\n",
    "    \n",
    "    return mainText\n",
    "    \n",
    "\n",
    "myAnalysis = []\n",
    "count = 0\n",
    "while (count < documentCount):\n",
    "    # Cluster, Original Category, File Reference, Keywords\n",
    "    myAnalysis.append([])\n",
    "    myAnalysis[count].append(clusters[count])\n",
    "    myAnalysis[count].append(documentReference[count][0])\n",
    "    myAnalysis[count].append(documentReference[count][1])\n",
    "    keywords = extractKeywords(documentReference[count][1])\n",
    "    myAnalysis[count].append(keywords)\n",
    "    count = count + 1\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "myLabels = ['Cluster', 'Category', 'File', 'Keywords']\n",
    "df = pd.DataFrame(myAnalysis, columns=myLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputFile = \"ClusterAnalysis.csv\"\n",
    "df = df.sort_values('Cluster', ascending = True)\n",
    "df.to_csv(outputFile, sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the key phrases from each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key words in cluster -  0\n",
      " election government party blair minister people labour tory plan tax year brown chancellor leader country law howard home lord report right service secretary campaign police issue bill world committee council britain time court system immigration kennedy case budget school week blunkett economy policy house voter vote pension child claim group \n",
      "\n",
      "Key words in cluster -  1\n",
      " people technology service phone firm year user game computer music software network system site company program security information consumer device player number mobile report data way market internet gadget sony machine broadband microsoft time virus apple industry world message website file customer search video console handset home sale medium project \n",
      "\n",
      "Key words in cluster -  2\n",
      " year company firm market share growth sale economy government bank profit price rate oil deal country euro month stock figure business analyst cost world airline china tax executive offer quarter job group car bid yukos president india dollar investment state spending court report demand debt interest time budget plan trade \n",
      "\n",
      "Key words in cluster -  3\n",
      " game player year england world side time champion injury team match cup club wale ireland chelsea season minute france win victory week goal nation coach williams seed race rugby manager title set scotland robinson championship jones liverpool league football try drug athens squad italy chance point record test day number \n",
      "\n",
      "Key words in cluster -  4\n",
      " film year award show star music actor band oscar album number director festival song office movie prize chart bbc singer game world comedy nomination book people week actress record rock winner role life category series fan time day box place artist ceremony group radio producer child act brother aviator hit \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def clusterKeyPatterns(text, wordcount):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    freqDist = nltk.FreqDist(tokens)\n",
    "    common = freqDist.most_common(wordcount)\n",
    "    \n",
    "    returnText = ''\n",
    "    for word in common:\n",
    "        returnText = returnText + ' ' + word[0]\n",
    "    \n",
    "    return returnText\n",
    "    \n",
    "clusterKeywords = []\n",
    "\n",
    "for num in range(0, num_clusters):\n",
    "    clusterKeywords.append([])\n",
    "    clusterKeywords[0].append('')\n",
    "\n",
    "for num in range(0, documentCount):\n",
    "    tmpStr = str(clusterKeywords[myAnalysis[num][0]]) + ' ' + str(myAnalysis[num][3])\n",
    "    clusterKeywords[myAnalysis[num][0]] = tmpStr\n",
    "\n",
    "for num in range(0, num_clusters):\n",
    "    # Obtain the top xx words in each cluster\n",
    "    listofwords = clusterKeyPatterns(str(clusterKeywords[num]), 50)\n",
    "    print(\"Key words in cluster - \", num)\n",
    "    print(listofwords, \"\\n\")   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above is perfect!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
