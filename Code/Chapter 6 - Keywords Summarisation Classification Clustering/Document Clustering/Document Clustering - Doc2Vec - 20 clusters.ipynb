{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2Vec for Document Clustering\n",
    "\n",
    "Dataset - http://mlg.ucd.ie/datasets/bbc.html\n",
    "\n",
    "Citation - D. Greene and P. Cunningham. \"Practical Solutions to the Problem of Diagonal Dominance in Kernel Document Clustering\", Proc. ICML 2006.\n",
    "\n",
    "Consists of 2225 documents from the BBC news website corresponding to stories in five topical areas from 2004-2005.\n",
    "Class Labels: 5 (business, entertainment, politics, sport, tech)\n",
    "- 510 business\n",
    "- 386 entertainment\n",
    "- 417 politics\n",
    "- 511 sports\n",
    "- 401 tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "documents = []\n",
    "documentReference = []\n",
    "documentCount = 0\n",
    "\n",
    "def dataCleanup(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\n', ' ')\n",
    "    \n",
    "    # Add space around punctuations\n",
    "    for char in ['.', '\"', ',', '(', ')', '!', '?', ';', ':']:\n",
    "        text = text.replace(char, ' ' + char + ' ')\n",
    "    \n",
    "    return text\n",
    "\n",
    "def buildDocument(folder, category):\n",
    "    allDocs = Path(folder).glob('**/*')\n",
    "    global documentCount\n",
    "\n",
    "    for doc in allDocs:\n",
    "        file = open(doc, \"r\")\n",
    "        data = file.read()\n",
    "        cleanData = dataCleanup(data)\n",
    "        \n",
    "        documents.append(cleanData)\n",
    "        \n",
    "        # Keep a record of category and filename\n",
    "        documentReference.append([])\n",
    "        documentReference[documentCount].append(category)\n",
    "        documentReference[documentCount].append(doc)\n",
    "        documentCount = documentCount + 1\n",
    "    \n",
    "    return\n",
    "\n",
    "buildDocument(\"bbc-fulltext/business\", \"business\")\n",
    "buildDocument(\"bbc-fulltext/entertainment\", \"entertainment\")\n",
    "buildDocument(\"bbc-fulltext/politics\", \"politics\")\n",
    "buildDocument(\"bbc-fulltext/sport\", \"sport\")\n",
    "buildDocument(\"bbc-fulltext/tech\", \"tech\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dhiraj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dhiraj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Doc2Vec Model Saved\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import doc2vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from collections import namedtuple\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "\n",
    "download('stopwords') # For stopword removal\n",
    "download('punkt') # For tokenizer\n",
    "\n",
    "def removeStopwords(text):\n",
    "    # Removing stopwords improved results with BBC news data, but test with and without stop words.\n",
    "    stop_words = stopwords.words('english')\n",
    "    text = [w for w in text if w not in stop_words]\n",
    "    text = [w for w in text if w.isalpha()]\n",
    "    return text\n",
    "\n",
    "def text2tokens(text):\n",
    "    text = text.lower()\n",
    "    wordList = word_tokenize(text)\n",
    "    wordList = removeStopwords(wordList)\n",
    "    return wordList\n",
    "\n",
    "doc2vec_corpus = []\n",
    "\n",
    "for i, text in enumerate(documents):\n",
    "    words = text2tokens(text)\n",
    "    tag = [i]\n",
    "    doc2vec_corpus.append(TaggedDocument(words=words, tags=tag))\n",
    "\n",
    "# The model parameters below can impact the outcome.\n",
    "# 1. Size - Vector size. 100 worked best with the BBC news data set. Tried various between 50 to 300 before choosing 100.\n",
    "# 2. Window - context window, i.e. the number of words on the left and right of a word that \n",
    "# defines a \"context\" for learning the meaning of the word. Context window of 1 gave the best result (tried between 1 and 10)\n",
    "# ..... probably due to the very small size of documents / vocabulary.\n",
    "\n",
    "model = doc2vec.Doc2Vec(doc2vec_corpus, size = 100, negative = 5, window = 1, iter = 20, min_count = 2, workers = 4, alpha=0.025, min_alpha=0.025)\n",
    "model.save(\"bbc_news_doc2vec.model\")\n",
    "print(\"Doc2Vec Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docVectors = []\n",
    "count = 0\n",
    "while (count < documentCount):\n",
    "    docVectors.append(model.docvecs[count])\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "num_clusters = 20\n",
    "\n",
    "km = KMeans(n_clusters = num_clusters, random_state = 99999)\n",
    "km.fit(docVectors)\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def wordFrequencyFilter(text, max_word_count, freq):\n",
    "    text = text.lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    wordList = []\n",
    "    for tag in tagged:\n",
    "        if ((tag[1] == 'NN') or (tag[1] == 'NNS') or (tag[1] == 'NNP') or (tag[1] == 'NNPS')):\n",
    "            tagLemma = lemmatizer.lemmatize(tag[0])\n",
    "            wordList.append(tagLemma)\n",
    "\n",
    "    freqDist = nltk.FreqDist(wordList)\n",
    "    common = freqDist.most_common(max_word_count)\n",
    "        \n",
    "    mainText = ''\n",
    "    \n",
    "    for word in common:\n",
    "        # Exclude words less than 2 characters long.\n",
    "        # Exclude words with frequency count greater than freq\n",
    "        # Only include alphabetic strings\n",
    "        \n",
    "        if ((word[0].isalpha()) and (len(word[0]) > 2) and ((word[1] < freq) == False)):\n",
    "            mainText = mainText + ' ' + word[0]\n",
    "    \n",
    "    return mainText\n",
    "\n",
    "def extractKeywords(file):\n",
    "    myFile = open(file, 'r')\n",
    "    data = myFile.read()\n",
    "    \n",
    "    # 10 most frequent words, minimum frequency 2\n",
    "    mainText = wordFrequencyFilter(data, 10, 2)\n",
    "    \n",
    "    return mainText\n",
    "    \n",
    "\n",
    "myAnalysis = []\n",
    "count = 0\n",
    "while (count < documentCount):\n",
    "    # Cluster, Original Category, File Reference, Keywords\n",
    "    myAnalysis.append([])\n",
    "    myAnalysis[count].append(clusters[count])\n",
    "    myAnalysis[count].append(documentReference[count][0])\n",
    "    myAnalysis[count].append(documentReference[count][1])\n",
    "    keywords = extractKeywords(documentReference[count][1])\n",
    "    myAnalysis[count].append(keywords)\n",
    "    count = count + 1\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "myLabels = ['Cluster', 'Category', 'File', 'Keywords']\n",
    "df = pd.DataFrame(myAnalysis, columns=myLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputFile = \"ClusterAnalysis.csv\"\n",
    "df = df.sort_values('Cluster', ascending = True)\n",
    "df.to_csv(outputFile, sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the key phrases from each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key words in cluster -  0\n",
      " firm share profit \n",
      "\n",
      "Key words in cluster -  1\n",
      " film award star show \n",
      "\n",
      "Key words in cluster -  2\n",
      " government lord law home right \n",
      "\n",
      "Key words in cluster -  3\n",
      " yukos oil firm court \n",
      "\n",
      "Key words in cluster -  4\n",
      " country government minister \n",
      "\n",
      "Key words in cluster -  5\n",
      " champion match seed \n",
      "\n",
      "Key words in cluster -  6\n",
      " technology game sony \n",
      "\n",
      "Key words in cluster -  7\n",
      " service phone broadband \n",
      "\n",
      "Key words in cluster -  8\n",
      " security software user program \n",
      "\n",
      "Key words in cluster -  9\n",
      " club game player chelsea champion \n",
      "\n",
      "Key words in cluster -  10\n",
      " court drug case charge \n",
      "\n",
      "Key words in cluster -  11\n",
      " england game wale ireland france \n",
      "\n",
      "Key words in cluster -  12\n",
      " government plan tax \n",
      "\n",
      "Key words in cluster -  13\n",
      " game time title phone \n",
      "\n",
      "Key words in cluster -  14\n",
      " site search information blog \n",
      "\n",
      "Key words in cluster -  15\n",
      " growth economy rate market \n",
      "\n",
      "Key words in cluster -  16\n",
      " election labour blair party brown \n",
      "\n",
      "Key words in cluster -  17\n",
      " party election leader minister \n",
      "\n",
      "Key words in cluster -  18\n",
      " music band album song \n",
      "\n",
      "Key words in cluster -  19\n",
      " minute penalty try goal ball \n",
      "\n"
     ]
    }
   ],
   "source": [
    "customStopwords = ['man', 'woman', 'men', 'women', 'year', 'people', 'male', 'female', \n",
    "                   'world', 'month', 'week', 'year', 'company']\n",
    "\n",
    "def clusterKeyPatterns(text, wordcount):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    freqDist = nltk.FreqDist(tokens)\n",
    "    common = freqDist.most_common(wordcount)\n",
    "    \n",
    "    returnText = ''\n",
    "    for word in common:\n",
    "        if ((word[0] not in customStopwords) and (word[1] > 1)):\n",
    "            returnText = returnText + ' ' + word[0]\n",
    "    \n",
    "    return returnText\n",
    "    \n",
    "clusterKeywords = []\n",
    "\n",
    "for num in range(0, num_clusters):\n",
    "    clusterKeywords.append([])\n",
    "    clusterKeywords[0].append('')\n",
    "\n",
    "for num in range(0, documentCount):\n",
    "    tmpStr = str(clusterKeywords[myAnalysis[num][0]]) + ' ' + str(myAnalysis[num][3])\n",
    "    clusterKeywords[myAnalysis[num][0]] = tmpStr\n",
    "\n",
    "for num in range(0, num_clusters):\n",
    "    # Obtain the top xx words in each cluster\n",
    "    listofwords = clusterKeyPatterns(str(clusterKeywords[num]), 5)\n",
    "    print(\"Key words in cluster - \", num)\n",
    "    print(listofwords, \"\\n\")   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above is perfect!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
