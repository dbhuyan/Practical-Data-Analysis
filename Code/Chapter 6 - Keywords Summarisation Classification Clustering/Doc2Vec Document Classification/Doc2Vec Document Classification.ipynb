{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2Vec for document classification\n",
    "\n",
    "Dataset - http://mlg.ucd.ie/datasets/bbc.html\n",
    "\n",
    "Citation - D. Greene and P. Cunningham. \"Practical Solutions to the Problem of Diagonal Dominance in Kernel Document Clustering\", Proc. ICML 2006.\n",
    "\n",
    "Consists of 2225 documents from the BBC news website corresponding to stories in five topical areas from 2004-2005.\n",
    "Class Labels: 5 (business, entertainment, politics, sport, tech)\n",
    "\n",
    "- 510 business\n",
    "- 386 entertainment\n",
    "- 417 politics\n",
    "- 511 sports\n",
    "- 401 tech\n",
    "\n",
    "We will add the first following documents to our corpus. The remaining I will use for testing purposes.\n",
    "- 500 business\n",
    "- 350 entertainment\n",
    "- 400 politics\n",
    "- 500 sports\n",
    "- 390 tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "trainingDocuments = []\n",
    "testingDocuments = []\n",
    "trainCount = 0\n",
    "testCount = 0\n",
    "\n",
    "sample = {'business': 500, 'entertainment': 350, 'politics': 400, 'sport': 500, 'tech': 390}\n",
    "\n",
    "def dataSetup(folder, category):\n",
    "    size = sample[category]\n",
    "    allDocs = Path(folder).glob('**/*')\n",
    "    global trainCount\n",
    "    global testCount\n",
    "    count = 0\n",
    "\n",
    "    for news in allDocs:\n",
    "        file = open(news, \"r\")\n",
    "        data = file.read()\n",
    "        \n",
    "        if (count < size):\n",
    "            trainingDocuments.append([])\n",
    "            trainingDocuments[trainCount].append(category)\n",
    "            trainingDocuments[trainCount].append(data)\n",
    "            trainCount = trainCount + 1\n",
    "            count = count + 1\n",
    "        else:\n",
    "            testingDocuments.append([])\n",
    "            testingDocuments[testCount].append(category)\n",
    "            testingDocuments[testCount].append(data)\n",
    "            testCount = testCount + 1\n",
    "\n",
    "    return\n",
    "\n",
    "businessText = dataSetup(\"bbc-fulltext/business\", \"business\")\n",
    "entertainmentText = dataSetup(\"bbc-fulltext/entertainment\", \"entertainment\")\n",
    "politicsText = dataSetup(\"bbc-fulltext/politics\", \"politics\")\n",
    "sportText = dataSetup(\"bbc-fulltext/sport\", \"sport\")\n",
    "techText = dataSetup(\"bbc-fulltext/tech\", \"tech\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dhiraj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dhiraj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import doc2vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from collections import namedtuple\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "\n",
    "download('stopwords') # For stopword removal\n",
    "download('punkt') # For tokenizer\n",
    "\n",
    "def removeStopwords(text):\n",
    "    # Removing stopwords improved results with BBC news data, but test with and without stop words.\n",
    "    stop_words = stopwords.words('english')\n",
    "    text = [w for w in text if w not in stop_words]\n",
    "    text = [w for w in text if w.isalpha()]\n",
    "    return text\n",
    "\n",
    "def text2tokens(text):\n",
    "    text = text.lower()\n",
    "    wordList = word_tokenize(text)\n",
    "    wordList = removeStopwords(wordList)\n",
    "    return wordList\n",
    "\n",
    "training_corpus = []\n",
    "testing_corpus = []\n",
    "\n",
    "for i, record in enumerate(trainingDocuments):\n",
    "    words = text2tokens(record[1])\n",
    "    tag = [record[0]] # IMPORTANT - I am using the news category as the document tag for training purpose!\n",
    "    training_corpus.append(TaggedDocument(words=words, tags=tag))\n",
    "\n",
    "for i, record in enumerate(testingDocuments):\n",
    "    words = text2tokens(record[1])\n",
    "    testing_corpus.append([])\n",
    "    testing_corpus[i].append(record[0])\n",
    "    testing_corpus[i].append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec Model Saved\n"
     ]
    }
   ],
   "source": [
    "# The model parameters below can impact the outcome.\n",
    "# 1. Size - Vector size. 100 worked best with the BBC news data set. Tried various between 50 to 300 before choosing 100.\n",
    "# 2. Window - context window, i.e. the number of words on the left and right of a word that \n",
    "# defines a \"context\" for learning the meaning of the word. Context window of 1 gave the best result (tried between 1 and 10)\n",
    "# ..... probably due to the very small size of documents / vocabulary.\n",
    "\n",
    "model = doc2vec.Doc2Vec(training_corpus, size = 100, negative=5, window = 1, iter = 20, min_count = 2, workers = 4, alpha=0.025, min_alpha=0.025)\n",
    "model.save(\"bbc_news_doc2vec.model\")\n",
    "print(\"Doc2Vec Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifyDoc(doc, category):\n",
    "    model.random.seed(0) # Force model to use a static seed rather than a random one.\n",
    "    new_vec = model.infer_vector(doc)\n",
    "    sims = model.docvecs.most_similar(positive=[new_vec], topn=5)\n",
    "    \n",
    "    print(\"Test document category - \", category)\n",
    "    print(\"Similarity results -\")\n",
    "    for neighbor in sims:\n",
    "        print(neighbor)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test document category -  entertainment\n",
      "Similarity results -\n",
      "('politics', 0.5703830122947693)\n",
      "('tech', 0.5324764251708984)\n",
      "('entertainment', 0.5239570736885071)\n",
      "('sport', 0.42939186096191406)\n",
      "('business', 0.4044690728187561)\n"
     ]
    }
   ],
   "source": [
    "testCategory = testing_corpus[10][0]\n",
    "testDoc = testing_corpus[10][1]\n",
    "classifyDoc(testDoc, testCategory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test document category -  tech\n",
      "Similarity results -\n",
      "('tech', 0.642406702041626)\n",
      "('politics', 0.4994729459285736)\n",
      "('sport', 0.44506770372390747)\n",
      "('business', 0.43930721282958984)\n",
      "('entertainment', 0.39877429604530334)\n"
     ]
    }
   ],
   "source": [
    "testCategory = testing_corpus[80][0]\n",
    "testDoc = testing_corpus[80][1]\n",
    "classifyDoc(testDoc, testCategory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test document category -  sport\n",
      "Similarity results -\n",
      "('sport', 0.8286051750183105)\n",
      "('politics', 0.588887095451355)\n",
      "('tech', 0.5434785485267639)\n",
      "('entertainment', 0.523962676525116)\n",
      "('business', 0.451291024684906)\n"
     ]
    }
   ],
   "source": [
    "testCategory = testing_corpus[70][0]\n",
    "testDoc = testing_corpus[70][1]\n",
    "classifyDoc(testDoc, testCategory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test document category -  business\n",
      "Similarity results -\n",
      "('business', 0.614448070526123)\n",
      "('sport', 0.5170470476150513)\n",
      "('politics', 0.49962225556373596)\n",
      "('tech', 0.4433245360851288)\n",
      "('entertainment', 0.41390979290008545)\n"
     ]
    }
   ],
   "source": [
    "testCategory = testing_corpus[0][0]\n",
    "testDoc = testing_corpus[0][1]\n",
    "classifyDoc(testDoc, testCategory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test document category -  politics\n",
      "Similarity results -\n",
      "('politics', 0.7728608250617981)\n",
      "('tech', 0.5455231666564941)\n",
      "('entertainment', 0.5144734382629395)\n",
      "('business', 0.504486620426178)\n",
      "('sport', 0.5038948655128479)\n"
     ]
    }
   ],
   "source": [
    "testCategory = testing_corpus[60][0]\n",
    "testDoc = testing_corpus[60][1]\n",
    "classifyDoc(testDoc, testCategory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correctly classified documents -  81\n",
      "Number of incorrectly classified documents -  4\n"
     ]
    }
   ],
   "source": [
    "correct_classification = 0\n",
    "incorrect_classification = 0\n",
    "\n",
    "for i, record in enumerate(testing_corpus):\n",
    "    model.random.seed(0)\n",
    "    new_vec = model.infer_vector(record[1])\n",
    "    sims = model.docvecs.most_similar(positive=[new_vec], topn=5)\n",
    "    if (sims[0][0] == testing_corpus[i][0]):\n",
    "        correct_classification = correct_classification + 1\n",
    "    else:\n",
    "        incorrect_classification = incorrect_classification + 1\n",
    "\n",
    "print(\"Number of correctly classified documents - \", correct_classification)\n",
    "print(\"Number of incorrectly classified documents - \", incorrect_classification)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
